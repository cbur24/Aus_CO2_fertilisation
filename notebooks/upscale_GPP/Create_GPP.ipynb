{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long-term AusEFlux GPP (1982-2022)\n",
    "\n",
    "Using a similar process as [AusEFlux](https://github.com/cbur24/AusEFlux/tree/master), this notebook creates GPP for Australia through the full length of the AVHHR and MODIS NDVI archive (i.e., 1982-2022). It relies on the [AusENDVI](https://github.com/cbur24/AusENDVI) dataset to provide the foundations of the predictions.\n",
    "\n",
    "***\n",
    "<!-- **Ideal compute environment:**\n",
    "\n",
    "Assuming 500m resolution\n",
    "\n",
    "- NCI's 'hugemem' queue\n",
    "- X-large (24 cores, 765GiB) #mostly for combining ensembles\n",
    "- Python 3.10.0\n",
    "- Python venv: `/g/data/xc0/project/AusEFlux/env/py310`\n",
    "- Storage Folders: `gdata/ub8+gdata/xc0` -->\n",
    "***\n",
    "<!-- > **Expected completion time to run all steps: ~3 hours** -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and set up Dask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.append('/g/data/xc0/project/AusEFlux/src/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up project directory structure\n",
    "\n",
    "This workflow assumes a specific file/folder structure, here we create that folder structure to support the rest of the process.\n",
    "\n",
    "Below, enter the `root directory location` where project results and data are stored, and determine the `target_grid` resolution (the spatial resolution of the final predictions, options are either '5km' or '1km').\n",
    "\n",
    "If the folders already exist then no directories will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base='/g/data/os22/chad_tmp/Aus_CO2_fertilisation/notebooks/upscale_GPP/'\n",
    "target_grid = '5km'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _utils import create_project_directories\n",
    "create_project_directories(root_dir=base, target_grid=target_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Extract training data\n",
    "\n",
    "Scrape the TERN server to extract all of the ozflux eddy covariance data, then append remote sensing data by using the coordinates of the flux tower to extract pixel values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Parameters\n",
    "\n",
    "* `version`: Version of OzFlux datasets to use, always has the form 'YYYY_v[number]'\n",
    "* `level`: What level of OzFlux data to use, level 6 is the highest level and has been pre-processed to 'analysis ready'\n",
    "* `type` : Ozflux data comes as either 'default' or 'site_pi' depending on how it was processed.\n",
    "* `rs_data_folder`: Where are the spatiotemporally harmonised and stacked feature layers that we will append to the EC data? The code simply loops through all netcdf files and appends the data. We can filter for features later on.\n",
    "* `save_ec_data`: If this variables is not 'None', then the EC netcdf files will be exported to this folder.\n",
    "* `export_path`: Where should we save the .csv files that contain the EC and RS data? i.e. this is our training data\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version='2023_v1'\n",
    "level='L6'\n",
    "type='default'\n",
    "target_grid='5km'\n",
    "base='/g/data/os22/chad_tmp/Aus_CO2_fertilisation/notebooks/upscale_GPP/'\n",
    "rs_data_folder=f'{base}data/{target_grid}/'\n",
    "save_ec_data=f'{base}data/ozflux_netcdf/'\n",
    "export_path=f'{base}data/training_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/g/data/xc0/project/AusEFlux/src/')\n",
    "from _training import extract_ozflux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_ozflux(\n",
    "    version=version,\n",
    "    level=level,\n",
    "    type=type,\n",
    "    rs_data_folder=rs_data_folder,\n",
    "    save_ec_data=None,\n",
    "    export_path=export_path,\n",
    "    return_coords=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a plot of all the OzFlux sites (Optional)\n",
    "\n",
    "This is helpful in ensuring the site locations are in the correct places - sometimes OzFlux coordinates are incorrect.\n",
    "\n",
    "We also export a .csv with the site locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_export = '/g/data/os22/chad_tmp/Aus_CO2_fertilisation/notebooks/upscale_GPP/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import contextily as ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sites = os.listdir(export_path)\n",
    "\n",
    "td = []\n",
    "for site in sites:\n",
    "    if '.csv' in site:\n",
    "        xx = pd.read_csv(export_path+site)\n",
    "        xx['site'] = site[0:-4]\n",
    "        xx = xx[['site', 'x_coord', 'y_coord']]\n",
    "        xx=xx.head(1)\n",
    "        td.append(xx)\n",
    "\n",
    "df = pd.concat(td).dropna()\n",
    "print('n_sites:', len(df))\n",
    "\n",
    "#export site list to file\n",
    "df.to_csv(site_export+'ozflux_site_locations.csv')\n",
    "\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df, geometry=gpd.points_from_xy(df.x_coord, df.y_coord), crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "ax = gdf.plot(column='site', figsize=(10,10))\n",
    "gdf.apply(lambda x: ax.annotate(text=x['site'],\n",
    "            xy=x.geometry.centroid.coords[0],\n",
    "            ha='right', fontsize=8), axis=1);\n",
    "\n",
    "# Adding basemap might fail with max retries...something is wrong with contextily backend\n",
    "ctx.add_basemap(ax, source=ctx.providers.Esri.WorldImagery, crs='EPSG:4326', attribution='', attribution_size=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate ensemble of Models\n",
    "\n",
    "We will attempt to model a portion of the empirical uncertainty that comes from the training data. To do this, we will generate 15 models. For each iteration, two flux tower sites  will be removed from the training data and an LGBM and RF model will be fit on the remaining data.  This will result in 30 models that later we can use to make 30 predictions. The IQR envelope of our predictions will inform our uncertainity\n",
    "\n",
    "> Note, before running this section shutdown any dask cluster that is running using `client.shutdown()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_var = 'GPP' #ER NEE ET GPP\n",
    "n_iter = 200 #how many hyperparameter iterations to test for the final model fitting?\n",
    "n_models = 15 #how many iterations of models to create (iterations of training data)?\n",
    "n_cpus = 32\n",
    "\n",
    "base='/g/data/os22/chad_tmp/Aus_CO2_fertilisation/notebooks/upscale_GPP/'\n",
    "\n",
    "ec_exclusions=['DalyUncleared', 'RedDirtMelonFarm', 'Loxton']\n",
    "\n",
    "modelling_vars = [\n",
    "                  'kNDVI_RS','kNDVI_anom_RS',#'f_total_RS',\n",
    "                  'trees_RS', 'grass_RS', 'bare_RS',\n",
    "                  'rain_RS', 'rain_cml3_RS', 'rain_anom_RS',\n",
    "                  'rain_cml3_anom_RS', 'rain_cml6_anom_RS', 'rain_cml12_anom_RS',\n",
    "                  'srad_RS', 'srad_anom_RS',\n",
    "                  'tavg_RS', 'tavg_anom_RS',\n",
    "                  'vpd_RS', 'vpd_anom_RS',\n",
    "                  'VegH_RS', 'site'\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comibine EC site data into a big pandas df------------------------\n",
    "sites = os.listdir(f'{base}data/training_data/')\n",
    "fluxes=['NEE_SOLO_EC','GPP_SOLO_EC','ER_SOLO_EC','ET_EC']\n",
    "td = []\n",
    "for site in sites:\n",
    "    if '.csv' in site:\n",
    "        if any(exc in site for exc in ec_exclusions): #don't load the excluded sites\n",
    "            print('skip', site[0:-4])\n",
    "            continue\n",
    "        else:\n",
    "            xx = pd.read_csv(f'{base}data/training_data/{site}',\n",
    "                             index_col='time', parse_dates=True)\n",
    "            xx['site'] = site[0:-4]\n",
    "            xx = xx[fluxes+modelling_vars]\n",
    "            td.append(xx)\n",
    "\n",
    "ts = pd.concat(td).dropna() #we'll use this later\n",
    "\n",
    "# convert pandas df into sklearn X, y --------------------------\n",
    "xx = []\n",
    "yy = []\n",
    "for t in td:    \n",
    "    t = t.dropna()  # remove NaNS\n",
    "    df = t.drop(['NEE_SOLO_EC','GPP_SOLO_EC','ER_SOLO_EC'],\n",
    "                axis=1) # seperate carbon fluxes\n",
    "    \n",
    "    df = df[modelling_vars]\n",
    "    \n",
    "    if model_var == 'ET':\n",
    "        df_var=t[[model_var+'_EC', 'site']]\n",
    "    else:\n",
    "        df_var=t[[model_var+'_SOLO_EC', 'site']]\n",
    "    \n",
    "    x = df.reset_index(drop=True)\n",
    "    y = df_var.reset_index(drop=True)\n",
    "    xx.append(x)\n",
    "    yy.append(y)\n",
    "\n",
    "x = pd.concat(xx)\n",
    "y = pd.concat(yy)\n",
    "print(x.shape)\n",
    "\n",
    "#export features list ----------------------------------\n",
    "textfile = open(f'{base}results/variables.txt', 'w')\n",
    "for element in x.columns:\n",
    "    textfile.write(element + \",\")\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Step 2\n",
    "\n",
    "> Note, it will take several hours to create 30 unique models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/g/data/xc0/project/AusEFlux/src/')\n",
    "from _ensemble_modelling import ensemble_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_models(\n",
    "    base=base,\n",
    "    model_var=model_var,\n",
    "    x=x,\n",
    "    y=y,\n",
    "    n_cpus=n_cpus,\n",
    "    n_iter=n_iter,\n",
    "    n_models=n_models,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create validaton plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/g/data/xc0/project/AusEFlux/src/')\n",
    "from _ensemble_modelling import validation_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_plots(\n",
    "    base=base,\n",
    "    model_var=model_var\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Create ensemble feature importance plots\n",
    "\n",
    "> Note, the RF models are very slow to process, so this can take several hours to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _ensemble_modelling import ensemble_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_feature_importance(\n",
    "    base=base,\n",
    "    model_var=model_var,\n",
    "    x=x,\n",
    "    y=y,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Predict ensemble\n",
    "\n",
    "Using the ensemble of models, we will generate an ensemble of gridded predictions. Each p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Parameters\n",
    "\n",
    "* `model_var`: Which variable are we modelling? Must be one of 'GPP', 'ER', 'NEE', or 'ET'\n",
    "* `base`: Path to where the harmonised datasets output from Step 1 are stored. \n",
    "* `results_path`: Path to store temporally stacked netcdf files i.e. where the outputs of Step 2 will be stored\n",
    "* `year_start`: The first year in the series to predict. If predicting for a single year, make _year_start_ and _year_end_ the same.\n",
    "* `year_end`: The last year in the series to predict. If predicting for a single year, make _year_start_ and _year_end_ the same.\n",
    "* `models_folder`: where are the models stored?\n",
    "* `features_list`: Where are the list of features used by the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_var = 'GPP'\n",
    "year_start, year_end=1982, 2022\n",
    "target_grid='5km'\n",
    "base='/g/data/os22/chad_tmp/Aus_CO2_fertilisation/notebooks/upscale_GPP/'\n",
    "results_path = f'{base}results/predictions/historical/{model_var}/'\n",
    "models_folder = f'{base}results/models/ensemble/{model_var}/'\n",
    "prediction_data=f'{base}data/{target_grid}'\n",
    "features_list = f'{base}results/variables.txt'\n",
    "masking_vars=['VegH','NDVI', 'rain_anom', 'tavg']\n",
    "n_workers=26\n",
    "memory_limit='120GiB'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_list = [models_folder+file for file in os.listdir(models_folder) if file.endswith(\".joblib\")]\n",
    "model_list.sort()\n",
    "os.chdir(base) #so o,e files get spit out here.\n",
    "\n",
    "#submit each model to gadi seperately for prediction\n",
    "for m in model_list:\n",
    "    print(m.split('/')[-1].split('.')[0])\n",
    "    \n",
    "    os.system(f\"qsub -v model_path={m},model_var={model_var},year_start={year_start},year_end={year_end},target_grid={target_grid},results_path={results_path},prediction_data={prediction_data},features_list={features_list},n_workers={n_workers},memory_limit={memory_limit} /g/data/xc0/project/AusEFlux/src/_qsub_ensemble_member.sh\"\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('/g/data/xc0/project/AusEFlux/src/')\n",
    "# from _utils import start_local_dask\n",
    "\n",
    "# import os\n",
    "# from _ensemble_prediction import predict_ensemble\n",
    "\n",
    "# start_local_dask(\n",
    "#         n_workers=26,\n",
    "#         threads_per_worker=1,\n",
    "#         memory_limit='115GiB'\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import os\n",
    "# #paths to models\n",
    "# model_list = [models_folder+file for file in os.listdir(models_folder) if file.endswith(\".joblib\")]\n",
    "# model_list.sort()\n",
    "\n",
    "# for m in model_list:\n",
    "#     print(m.split('/')[-1].split('.')[0])\n",
    "    \n",
    "#     predict_ensemble(\n",
    "#        prediction_data=prediction_data,\n",
    "#        model_path=m,\n",
    "#        model_var=model_var,\n",
    "#        features_list=features_list,\n",
    "#        results_path=results_path,\n",
    "#        year_start=year_start,\n",
    "#        year_end=year_end,\n",
    "#        target_grid=target_grid,\n",
    "#        masking_vars=['VegH','NDVI', 'rain_anom', 'tavg'],\n",
    "#        compute_early=True,\n",
    "#        verbose=True\n",
    "#     )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Combine ensembles\n",
    "\n",
    "Ran an ensemble of predictions, now we need to compute the ensemble median and the uncertainty range.\n",
    "\n",
    "This step will also output production ready datasets with appropriate metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Parameters\n",
    "\n",
    "* `model_var`: Which variable are we combining? Must be one of 'GPP'\n",
    "* `base`: Path to where the modelling/data etc is occuring. We build the other path strings from the 'base' path to reduce the length of path strings.\n",
    "* `results_path`: Path where final AusEFlux datasets will be output.\n",
    "* `year_start`: The first year in the series. If running for a single year, make _year_start_ and _year_end_ the same.\n",
    "* `year_end`: The last year in the series. If running for a single year, make _year_start_ and _year_end_ the same.\n",
    "* `quantiles`: What quantiles are we using to determine the middle value and uncertainty range? The default is 0.05 and 0.95 for the uncertainty envelope, and 0.5 (median) for the middle estimate. You're advised not to change these.\n",
    "* `predictions_folder`: where are the ensemble predictions stored? Those output from the previous step.\n",
    "\n",
    "> There are also several metadata fields (e.g. `full_name`, `units`) that will change with the variable being modelled. Make sure you update these for each model run as these atttributes are appended to the exported netcdf files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base='/g/data/os22/chad_tmp/Aus_CO2_fertilisation/notebooks/upscale_GPP/'\n",
    "model_var = 'GPP'\n",
    "results_path = f'{base}results/AusEFlux/{model_var}/'\n",
    "year_start, year_end=1982,2022\n",
    "target_grid='5km'\n",
    "quantiles=[0.25,0.5,0.75] # interquartile range\n",
    "predictions_folder= f'{base}results/predictions/historical/{model_var}/'\n",
    "\n",
    "dask_chunks=dict(x=250, y=250, time=-1) #small spatial chuncks for 500m res.\n",
    "\n",
    "# metadata for netcdf attributes\n",
    "full_name = 'Gross Primary Productivity'\n",
    "version = 'v0.2'\n",
    "crs='EPSG:4326'\n",
    "units = 'gC/m\\N{SUPERSCRIPT TWO}/month'\n",
    "description = f'AusEFlux {full_name} is created by empirically upscaling the OzFlux eddy covariance network using machine learning methods coupled with climate and remote sensing datasets. The estimates provided within this dataset were extracted from an ensemble of predictions and represent the median and uncertainty range.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create attributes dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "attrs_dict={}\n",
    "attrs_dict['nodata'] = np.nan\n",
    "attrs_dict['crs'] = crs\n",
    "attrs_dict['short_name'] = model_var\n",
    "attrs_dict['long_name'] = full_name\n",
    "attrs_dict['units'] = units\n",
    "attrs_dict['version'] = version\n",
    "attrs_dict['description'] = description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/g/data/xc0/project/AusEFlux/src/')\n",
    "from _combine_ensemble import combine_ensemble\n",
    "from _utils import start_local_dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_local_dask(\n",
    "        n_workers=12,\n",
    "        threads_per_worker=1,\n",
    "        memory_limit='240GiB'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### %%time\n",
    "combine_ensemble(\n",
    "    model_var=model_var,\n",
    "    results_path=results_path,\n",
    "    dask_chunks=dask_chunks,\n",
    "    predictions_folder=predictions_folder,\n",
    "    year_start=year_start,\n",
    "    year_end=year_end,\n",
    "    attrs=attrs_dict,\n",
    "    target_grid=target_grid,\n",
    "    quantiles=quantiles,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a single netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from odc.geo.xr import assign_crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'v0.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base='/g/data/os22/chad_tmp/Aus_CO2_fertilisation/notebooks/upscale_GPP/'\n",
    "folder = base+f'results/AusEFlux/GPP/'\n",
    "files = [f'{folder}/{i}' for i in os.listdir(folder) if i.endswith(\".nc\")]\n",
    "files.sort()\n",
    "\n",
    "#combine annual files into one file\n",
    "ds = xr.open_mfdataset(files).sel(time=slice('1982','2022'))\n",
    "ds = assign_crs(ds, crs='EPSG:4326')\n",
    "ds.attrs['nodata'] = np.nan\n",
    "\n",
    "del ds['GPP_median'].attrs['grid_mapping']\n",
    "ds = ds['GPP_median']\n",
    "ds.name = 'GPP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_netcdf(f'/g/data/os22/chad_tmp/Aus_CO2_fertilisation/data/AusEFlux_versions/AusEFlux_GPP_5km_1982_2022_{version}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
